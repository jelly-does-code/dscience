{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rabbitmetrics/cx-analytics/blob/main/notebooks/cx-analytics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GVUQKzjI9DD6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import chromadb\n",
    "\n",
    "# Imports needed for vector store with huggingface\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Imports needed for vector store/model predictions with distilbert\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xfMx26bI9UYC"
   },
   "outputs": [],
   "source": [
    "# Embedding function for Distilbert\n",
    "tokenizer_distilbert = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model_distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def create_embedding_distilbert(text):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer_distilbert(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    # Get model outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model_distilbert(**inputs)\n",
    "    # Take the mean of the last hidden state to get the sentence embedding\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=\".\")\n",
    "\n",
    "collection_hf = client.get_or_create_collection(\"statbel_hf\")\n",
    "collection_distilbert = client.get_or_create_collection(\"statbel_distilbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make vector store statbel_huggingface with unnormalized huggingface embedding\n",
    "chroma_db_path = \"./chroma.sqlite3\"\n",
    "hf_embeddings_model = HuggingFaceEmbeddings()\n",
    "\n",
    "\n",
    "# Check if the ChromaDB file exists. If not, build it\n",
    "if not os.path.exists(chroma_db_path):\n",
    "    # Load the context data\n",
    "    df = pd.read_csv('input_data/soc_sample.csv', header=0)\n",
    "    df = df[df.iloc[:, 0].str.startswith('in the city of Halle')]\n",
    "\n",
    "\n",
    "    # Huggingface embeddings\n",
    "    df['embeddings_hf'] = df['text'].apply(lambda x: hf_embeddings_model.embed_query(x))\n",
    "    \n",
    "    # DistilBERT embeddings\n",
    "    df['embeddings_distilbert'] = df['text'].apply(lambda x: create_embedding_distilbert(x))\n",
    "\n",
    "    ids = [str(index) for index in df.index]  # Convert index to string for IDs\n",
    "\n",
    "    collection_hf = client.get_or_create_collection(name=\"statbel_hf\")\n",
    "    collection_hf.add(\n",
    "        ids=ids,  # Unique identifiers for each document\n",
    "        documents=df['text'].tolist(), \n",
    "        embeddings=df['embeddings_hf'].tolist(),\n",
    "        metadatas=[{\"index\": index} for index in df.index]  # Optional metadata\n",
    "        )\n",
    "    \n",
    "    # Make vector store statbel_distilbert with unnormalized distilbert embedding\n",
    "    collection_distilbert = client.get_or_create_collection(name=\"statbel_distilbert\")\n",
    "    collection_distilbert.add(\n",
    "        ids=ids,  # Unique identifiers for each document\n",
    "        documents=df['text'].tolist(),  # The text you want to store\n",
    "        embeddings=df['embeddings_distilbert'].tolist(),  # The corresponding embeddings\n",
    "        metadatas=[{\"index\": index} for index in df.index]  # Optional metadata\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface:\n",
      "in the city of Halle the number of males born in bel unmarried and age 52 is 75\n",
      "in the city of Halle the number of males born in bel unmarried and age 65 is 30\n",
      "in the city of Halle the number of males born in bel unmarried and age 53 is 80\n",
      "in the city of Halle the number of males born in bel unmarried and age 49 is 78\n",
      "in the city of Halle the number of males born in bel unmarried and age 50 is 87\n",
      "in the city of Halle the number of males born in bel unmarried and age 59 is 47\n",
      "in the city of Halle the number of males born in bel unmarried and age 63 is 30\n",
      "in the city of Halle the number of males born in bel unmarried and age 51 is 69\n",
      "in the city of Halle the number of males born in bel unmarried and age 47 is 78\n",
      "in the city of Halle the number of males born in bel unmarried and age 60 is 37\n",
      "\n",
      "distilbert:\n",
      "in the city of Halle the number of males born in bel unmarried and age 65 is 30\n",
      "in the city of Halle the number of males born in bel unmarried and age 61 is 40\n",
      "in the city of Halle the number of males born in bel unmarried and age 60 is 37\n",
      "in the city of Halle the number of males born in bel divorced and age 65 is 31\n",
      "in the city of Halle the number of males born in bel widow and age 70 is 8\n",
      "in the city of Halle the number of males born in bel married and age 82 is 50\n",
      "in the city of Halle the number of males born in bel widow and age 65 is 9\n",
      "in the city of Halle the number of males born in bel married and age 65 is 120\n",
      "in the city of Halle the number of males born in bel divorced and age 60 is 45\n",
      "in the city of Halle the number of males born in bel widow and age 66 is 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start asking some questions ..\n",
    "query = \"in the city of Halle how many males of age 65 which are born in bel and married?\"\n",
    "query = \"in the city of Halle how many males of age 65 are there\"\n",
    "\n",
    "\n",
    "def get_context(query, model):\n",
    "    if model == 'huggingface':\n",
    "        query_embedding = hf_embeddings_model.embed_query(query)\n",
    "        results = collection_hf.query(\n",
    "                                      query_embeddings=[query_embedding],  \n",
    "                                      n_results=10\n",
    "                                     )\n",
    "    elif model == 'distilbert':\n",
    "        query_embedding = create_embedding_distilbert(query)\n",
    "        results = collection_distilbert.query(\n",
    "                                              query_embeddings=[query_embedding],\n",
    "                                              n_results=10\n",
    "                                             ) \n",
    "        \n",
    "    context = \"\\n\".join(results['documents'][0])\n",
    "    print(f'{model}:\\n{context}\\n')\n",
    "    return context\n",
    "\n",
    "context_hf = get_context(query, 'huggingface')\n",
    "context_distilbert = get_context(query, 'distilbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 30\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForQuestionAnswering\n",
    "\n",
    "# Get the model's predictions\n",
    "inputs = tokenizer_distilbert(query, context_hf, return_tensors='pt', padding=True, truncation=True)\n",
    "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad')\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "\n",
    "# Get the start and end scores\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits\n",
    "\n",
    "\n",
    "# Get the most likely start and end of the answer\n",
    "start_index = torch.argmax(start_scores)\n",
    "end_index = torch.argmax(end_scores) + 1  # +1 because the end index is inclusive\n",
    "\n",
    "\n",
    "# Decode the answer\n",
    "answer_tokens = inputs['input_ids'][0][start_index:end_index]\n",
    "answer = tokenizer_distilbert.decode(answer_tokens)\n",
    "\n",
    "\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO6+Sy9CskVr7oBXABvSo8e",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
