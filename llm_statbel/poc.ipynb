{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GVUQKzjI9DD6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('input_data/soc_sample.csv', header=0)\n",
    "df = df[df.iloc[:, 0].str.startswith('in the city of Halle')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jelly/Sync/jelly/dev/dscience/venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Make vector store statbel with unnormalized huggingface embedding\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "collection_name=\"statbel\"\n",
    "\n",
    "recreate_vectorstore = 0\n",
    "if recreate_vectorstore:\n",
    "    try:\n",
    "        os.remove('chroma.sqlite3')\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    vector_store = Chroma(collection_name=collection_name, \n",
    "                      embedding_function=embedding_function,\n",
    "                      persist_directory=\".\")\n",
    "    \n",
    "    loader = DataFrameLoader(df, page_content_column=\"text\")\n",
    "    docs = loader.load()\n",
    "    \n",
    "    vector_store.add_documents(docs)\n",
    "else:\n",
    "    vector_store = Chroma(collection_name=collection_name, \n",
    "                      embedding_function=embedding_function,\n",
    "                      persist_directory=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the city of Halle the number of males born in bel married and age 85 is 57\n",
      "in the city of Halle the number of males born in bel married and age 84 is 57\n",
      "in the city of Halle the number of males born in bel married and age 35 is 77\n",
      "in the city of Halle the number of males born in bel married and age 82 is 50\n",
      "in the city of Halle the number of males born in bel married and age 81 is 75\n",
      "in the city of Halle the number of males born in bel married and age 83 is 59\n",
      "in the city of Halle the number of males born in bel married and age 65 is 120\n",
      "in the city of Halle the number of males born in bel married and age 77 is 95\n",
      "in the city of Halle the number of males born in bel married and age 38 is 95\n",
      "in the city of Halle the number of males born in bel married and age 86 is 46\n"
     ]
    }
   ],
   "source": [
    "# Start asking some questions ..\n",
    "query = \"in the city of Halle how many males of age 65 which are born in bel and married?\"\n",
    "\n",
    "results = vector_store.similarity_search(query, k=10)\n",
    "\n",
    "# Combine the retrieved documents to use as context\n",
    "context = \"\\n\".join(result.page_content.strip() for result in results)\n",
    "\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(result.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question based only on the following context: in the city of Halle the number of males born in bel married and age 85 is 57\n",
      "in the city of Halle the number of males born in bel married and age 84 is 57\n",
      "in the city of Halle the number of males born in bel married and age 35 is 77\n",
      "in the city of Halle the number of males born in bel married and age 82 is 50\n",
      "in the city of Halle the number of males born in bel married and age 81 is 75\n",
      "in the city of Halle the number of males born in bel married and age 83 is 59\n",
      "in the city of Halle the number of males born in bel married and age 65 is 120\n",
      "in the city of Halle the number of males born in bel married and age 77 is 95\n",
      "in the city of Halle the number of males born in bel married and age 38 is 95\n",
      "in the city of Halle the number of males born in bel married and age 86 is 46\n",
      "\n",
      "Question: in the city of Halle how many males of age 65 which are born in bel and married?\n"
     ]
    }
   ],
   "source": [
    "# Query the llm\n",
    "query_w_context = f\"\"\"Answer the question based only on the following context: {context}\n",
    "\n",
    "Question: {query}\"\"\"\n",
    "\n",
    "# Function to query local flask server llm (mistral-7b-orca)\n",
    "def query_llm(prompt):\n",
    "    url = \"http://192.168.0.23:8000/generate\"\n",
    "    response = requests.post(url, json={\"prompt\": prompt})\n",
    "    data = response.json()\n",
    "    return data\n",
    "\n",
    "response = query_llm(query_w_context)\n",
    "print(query_w_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated on cuda.\n",
      "\n",
      "Response: Based on the context, there are 120 males born in Halle who are males born in bel married and age 65.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processing_type = response[\"device\"]\n",
    "answer = response[\"response\"]['choices'][0]['text'].strip()\n",
    "\n",
    "output = f'''\n",
    "Generated on {processing_type}.\n",
    "\n",
    "Response: {answer}\n",
    "\n",
    "'''\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO6+Sy9CskVr7oBXABvSo8e",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
