{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5489c9ff-3e1f-4b23-877d-09371567cb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                           0       1            2            3       4  \\\n",
      "87953  in the city of Halle  female  born in bel      married  age 44   \n",
      "87954  in the city of Halle    male  born in bel      married  age 32   \n",
      "87955  in the city of Halle  female  born in bel      married  age 56   \n",
      "87956  in the city of Halle  female  born in bel      married  age 82   \n",
      "87957  in the city of Halle  female  born in bel      married  age 80   \n",
      "...                     ...     ...          ...          ...     ...   \n",
      "88954  in the city of Halle  female      foreign  not married  age 50   \n",
      "88955  in the city of Halle  female      foreign  not married  age 77   \n",
      "88956  in the city of Halle  female      foreign  not married  age 37   \n",
      "88957  in the city of Halle    male      foreign  not married  age 37   \n",
      "88958  in the city of Halle    male      foreign  not married  age 51   \n",
      "\n",
      "                             5  \n",
      "87953  number of people is 116  \n",
      "87954   number of people is 74  \n",
      "87955  number of people is 119  \n",
      "87956   number of people is 47  \n",
      "87957   number of people is 74  \n",
      "...                        ...  \n",
      "88954    number of people is 9  \n",
      "88955    number of people is 1  \n",
      "88956   number of people is 13  \n",
      "88957   number of people is 23  \n",
      "88958   number of people is 10  \n",
      "\n",
      "[1006 rows x 6 columns]>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Load your dataset\n",
    "dataset = pd.read_csv('input/soc_sample.csv', header=None)\n",
    "dataset = dataset[dataset.iloc[:, 0].str.contains('in the city of Halle', case=False)]\n",
    "\n",
    "print(dataset.head)\n",
    "# Initialize embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # Efficient, lightweight model\n",
    "\n",
    "# Prepare and embed data\n",
    "text_data = dataset.apply(lambda row: f\"{row.to_json()}\", axis=1).tolist()\n",
    "embeddings = embedding_model.encode(text_data, convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30ae7e42-032b-4dda-8a63-e3be2997ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "def retrieve_relevant_context(query, embeddings, text_data, embedding_model, top_k=10):\n",
    "    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "    similarities = cosine_similarity(query_embedding.unsqueeze(0), embeddings)\n",
    "    top_k_indices = torch.topk(similarities, top_k).indices\n",
    "    return [text_data[idx] for idx in top_k_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f293b6d-192c-4ce7-965d-483693a9960a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting tokens\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load a small local language model (DistilGPT-2 as an example)\n",
    "model_name = \"distilgpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set the pad token id if not already set (GPT models typically do not have a pad token)\n",
    "if tokenizer.pad_token is None:\n",
    "    print(\"setting tokens\")\n",
    "    tokenizer.pad_token = \"<|endoftext|>\"  # Set the pad token to the end of text token\n",
    "    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "\n",
    "def generate_answer(query, context):\n",
    "    input_text = f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    \n",
    "    # Tokenize the input text with padding, truncation, and return tensors\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,               # Ensures proper padding\n",
    "        truncation=True,            # Truncates to max length\n",
    "        max_length=500              # Set a maximum length to avoid excessive tokenization\n",
    "    )\n",
    "\n",
    "    # Generate the response using attention masks\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],        # Use 'input_ids' from the tokenizer output\n",
    "        attention_mask=inputs['attention_mask'],  # Include attention mask\n",
    "        max_length=600,             # Adjust length as needed\n",
    "        num_return_sequences=1,\n",
    "        max_new_tokens=100\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6240a634-d727-48a3-8b7e-0616d001d4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Both `max_new_tokens` (=100) and `max_length`(=600) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: {\"0\":\"in the city of Halle\",\"1\":\"male\",\"2\":\"born in bel\",\"3\":\"divorced\",\"4\":\"age 66\",\"5\":\"number of people is 32\"}\n",
      "{\"0\":\"in the city of Halle\",\"1\":\"male\",\"2\":\"born in bel\",\"3\":\"not married\",\"4\":\"age 66\",\"5\":\"number of people is 29\"}\n",
      "{\"0\":\"in the city of Halle\",\"1\":\"male\",\"2\":\"born in bel\",\"3\":\"not married\",\"4\":\"age 4\",\"5\":\"number of people is 217\"}\n",
      "{\"0\":\"in the city of Halle\",\"1\":\"male\",\"2\":\"born in bel\",\"3\":\"not married\",\"4\":\"age 17\",\"5\":\"number of people is 219\"}\n",
      "{\"0\":\"in the city of Halle\",\"1\":\"male\",\"2\":\"born in bel\",\"3\":\"divorced\",\"4\":\"age 71\",\"5\":\"number of people is 23\"}\n",
      "{\"0\":\"in the city of Halle\",\"1\":\"male\",\"2\":\"born in bel\",\"3\":\"divorced\",\"4\":\"age 68\",\"5\":\"number of people is 38\"}\n",
      "{\"0\":\"in the city of Halle\",\"1\":\"female\",\"2\":\"born in bel\",\"3\":\"not married\",\"4\":\"age 28\",\"5\":\"number of people is 194\"}\n",
      "{\"0\":\"in the city of Halle\",\"1\":\"male\",\"2\":\"born in bel\",\"3\":\"divorced\",\"4\":\"age 73\",\"5\":\"number of people is 18\"}\n",
      "{\"0\":\"in the city of Halle\",\"1\":\"male\",\"2\":\"born in bel\",\"3\":\"divorced\",\"4\":\"age 72\",\"5\":\"number of people is 28\"}\n",
      "{\"0\":\"in the city of Halle\",\"1\":\"male\",\"2\":\"born in bel\",\"3\":\"not married\",\"4\":\"age 34\",\"5\":\"number of people is 153\"}\n",
      "\n",
      "Question: Please sum all relevant entries such that you can tell me how many males in Halle who were born in bel of age 30 are divorced\n",
      "Answer: Please sum all relevant entries such that you can tell me how many males in Halle who were born in bel of age 30 are divorced\n",
      "Answer: Please sum all relevant entries such that you can tell me how many males in Halle who were born in bel of age 30 are divorced\n",
      "Answer: Please sum all relevant entries such that you can tell me how many males in Halle who were born in bel of age 30 are divorced\n",
      "Answer: Please sum all relevant entries such that you can tell\n"
     ]
    }
   ],
   "source": [
    "def ask_question(query):\n",
    "    relevant_context = retrieve_relevant_context(query, embeddings, text_data, embedding_model)\n",
    "    context = \"\\n\".join(relevant_context)\n",
    "    answer = generate_answer(query, context)\n",
    "    return answer\n",
    "\n",
    "# Example usage\n",
    "question = \"Hany males in Halle who were born in bel of age 30 are divorced\"\n",
    "print(ask_question(question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365cf708-957f-4e72-9fe3-a6ec3c25127e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44308404-c2d1-4822-8dc1-a25b9d66cd9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32edb893-57f5-4d8f-ac07-96c83da26d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import faiss\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load your dataset (replace 'your_dataset.csv' with your actual dataset file)\n",
    "dataframe = pd.read_csv('input/soc_sample.csv')\n",
    "dataframe = dataframe[dataframe.iloc[:, 0].str.startswith('in the city of Halle')]\n",
    "\n",
    "# Initialize the embedding model (you can choose an appropriate model)\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f208806-0654-4812-8dbf-9ab02164bfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Embed the dataset\n",
    "# Concatenate all column values for each row into a single string\n",
    "text_data = dataframe.astype(str).agg(' '.join, axis=1).tolist()\n",
    "embeddings = embedding_model.encode(text_data, convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa55fac7-5666-4fc4-86b9-aae87cc1c42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Create a FAISS index for efficient similarity search\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)  # Using L2 distance\n",
    "index.add(embeddings.cpu().numpy())  # Add embeddings to the index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddcb55d1-5f90-4b16-a5de-a1c7012b1a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the language model\n",
    "model_name = \"distilgpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49a3498c-788d-4338-8cb6-768b051731e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the pad token id if not already set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = \"<|endoftext|>\"\n",
    "    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "\n",
    "def retrieve_relevant_context(query, embedding_model, index, text_data, top_k=3):\n",
    "    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "    query_embedding_np = query_embedding.cpu().numpy().reshape(1, -1)  # Ensure it's 2D (1, embedding_size)\n",
    "    _, top_k_indices = index.search(query_embedding_np, top_k)  # Retrieve top K indices\n",
    "    return [text_data[idx] for idx in top_k_indices[0]]\n",
    "\n",
    "def generate_answer(query):\n",
    "    # Step 3: Retrieve relevant context from the dataset\n",
    "    relevant_contexts = retrieve_relevant_context(query, embedding_model, index, text_data)\n",
    "    context = \"\\n\".join(relevant_contexts)  # Combine contexts for the language model\n",
    "\n",
    "    # Create a structured input\n",
    "    input_text = f\"Dataset context:\\n{context}\\n\\nQuestion about the dataset: {query}\\nAnswer:\"\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=600  # Adjust based on your needs\n",
    "    )\n",
    "\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        num_return_sequences=1,\n",
    "        max_new_tokens=10\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07871501-cf87-4d66-83a8-2efcd897fe31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset context:\n",
      "in the city of Halle male born in bel divorced age 73 number of people is 18\n",
      "in the city of Halle male born in bel divorced age 70 number of people is 18\n",
      "in the city of Halle male born in bel divorced age 64 number of people is 59\n",
      "\n",
      "Question about the dataset: Hany males in Halle who were born in bel of age 30 are divorced\n",
      "Answer:\n",
      "Question:\n",
      "Question:\n",
      "Question:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "question = \"Hany males in Halle who were born in bel of age 30 are divorced\"\n",
    "answer = generate_answer(question)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1f6c52-baef-409f-963c-7412cc553b87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
